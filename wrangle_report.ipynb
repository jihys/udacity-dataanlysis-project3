{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Wrangle and Analyze Data (WeRateDogs)\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">1. Introduction</a></li>\n",
    "<li><a href=\"#gathering\">2. Gathering</a></li>\n",
    "<li><a href=\"#assessing\">3. Assessing</a></li>\n",
    "<li><a href=\"#cleaning\">4. Cleaning</a></li>\n",
    "<li><a href=\"#visualization\">5. Analysis and Visualization</a></li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<a id='intro'></a>\n",
    "\n",
    "In this project, I wrangled **WeRateDogs** Twitter data to create interesting and trustworthy analyses and visualizations. Since the Twitter archive only contains very basic tweet information, I additionaly gathered data using Tweeter API and combined with the WeRateDogs Twitter data. The combined data was assessed and cleaned to get insightful analyses and visualizations. \n",
    "\n",
    "### Data \n",
    "#### WeRateDog Twitter Archive \n",
    "\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\" Of the 5000+ tweets, I have filtered for tweets with ratings only (there are 2356).\n",
    "![image.png](https://video.udacity-data.com/topher/2017/October/59dd4791_screenshot-2017-10-10-18.19.36/screenshot-2017-10-10-18.19.36.png)\n",
    "\n",
    "#### Additional Data via the Twitter API\n",
    "\n",
    "Retweet count and favorite count are very important information but these values are omitted. So I gathered these information through Twitter's API for all 5000+ tweet IDs within the enhanced tweetter archive file. \n",
    "\n",
    "#### Twitter Image Predictions File\n",
    "This file contains the dog breed classification results from a Nuerual Network model for every images in the WeRateDogs Twitter archive. This file has a table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images)\n",
    "![image.png](https://video.udacity-data.com/topher/2017/October/59dd4d2c_screenshot-2017-10-10-18.43.41/screenshot-2017-10-10-18.43.41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## 2. Gathering\n",
    "\n",
    "In this part I gathered data for this project.\n",
    "1. The **WeRateDogs Twitter Archive** data is saved as the `twitter_archive_enhanced.csv` file.\n",
    "2. **Twitter image prediction file** `image_predictions.tsv` is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv \n",
    "3. Each tweet's retweet count and favorite (\"like\") count at minimum, and any additional data you find interesting. \n",
    "Using the tweet IDs in the WeRateDogs Twitter archive, I **queryed the Twitter API** for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called `tweet_json.txt` file. Each tweet's JSON data was written to its own line. Then read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count. \n",
    "\n",
    "\n",
    "### 2.1 Gather & Check WeRateDogs Twitter Archive file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file and display first few lines\n",
    "df=pd.read_csv(\"twitter-archive-enhanced.csv\")\n",
    "df.head()\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of rows and null values, types etc.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Download Twitter Image Predictions File and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "#print(response.content)\n",
    "with open('image-predictions.tsv', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "image_pred=pd.read_csv(\"image-predictions.tsv\",sep='\\t')\n",
    "image_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of rows and null values, types etc.\n",
    "image_pred.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Crawl Twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "# These are hidden to comply with Twitter's API terms and conditions\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_secret = ''\n",
    "\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# NOTE TO STUDENT WITH MOBILE VERIFICATION ISSUES:\n",
    "# df_1 is a DataFrame with the twitter_archive_enhanced.csv file. You may have to\n",
    "# change line 17 to match the name of your DataFrame with twitter_archive_enhanced.csv\n",
    "# NOTE TO REVIEWER: this student had mobile verification issues so the following\n",
    "# Twitter API code was sent to this student from a Udacity instructor\n",
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "tweet_ids = df.tweet_id.values\n",
    "len(tweet_ids)\n",
    "\n",
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out failed queries.\n",
    "for key in fails_dict.keys():\n",
    "    print(key, fails_dict[key])\n",
    "len(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retry crawling for two tweet ids which were occured for connection errors. \n",
    "#Other errors represent that ID is no longer exist or I have no permission.\n",
    "\n",
    "failure_ids=[758740312047005698,676957860086095872]\n",
    "retry_fails_dict={}\n",
    "with open('tweet_json.txt', 'a') as outfile:\n",
    "    for tweet_id in failure_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            retry_fails_dict[tweet_id] = e\n",
    "            pass\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json = []\n",
    "# read in the json file line by line into a list\n",
    "with open(\"tweet_json.txt\") as file:\n",
    "    for line in file:\n",
    "        tweet_json.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame containing the relevant api data\n",
    "api_data = pd.DataFrame({'tweet_id': [i[\"id_str\"] for i in tweet_json], \n",
    "     'retweet_count': [i[\"retweet_count\"] for i in tweet_json], \n",
    "     'favorite_count': [i[\"favorite_count\"] for i in tweet_json], \n",
    "     'retweet_count' : [i[\"retweet_count\"] for i in tweet_json],\n",
    "     'retweeted' : [i[\"retweeted\"] for i in tweet_json],\n",
    "     'followers_count': [i[\"user\"][\"followers_count\"] for i in tweet_json], \n",
    "     'friends_count' :[i['user']['friends_count'] for i in tweet_json]               \n",
    "     })\n",
    "api_data.to_csv('api_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "## 3. Assessing\n",
    "\n",
    "I used Jupyter notebook and other tools(spreadsheet) to invistigate data first. And then later, progrmatically assessed data. \n",
    "\n",
    "Text column from twitter Archive seems to have mulitple information such as image url, review comments, ratings. This information later used to correct ratings value (rating_numerator & rating_denominator). \n",
    "\n",
    "Crawled Twitter Data seems to have incorrect informations which were due to limitation in my permission or subscription level. \n",
    "\n",
    "The detailed observation results are written in <a href=\"#observation\">3.3 Observation</a>\n",
    "\n",
    "### 3.1 Visual Assessment\n",
    "\n",
    "\n",
    "<img src=\"img/weratedog_raw.png\" alt=\"weratedog\" title=\"WeRateDog Twitter Archive\" width=\"800\" height=\"300\" />\n",
    "<img src=\"img/image_pred.png \" alt=\"image_pred\" title=\"Image Prediction\" width=\"800\" height=\"300\" />\n",
    "<img src=\"img/api-data.png \" alt=\"image_pred\" title=\"API Data\" width=\"700\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Programmatic assessment\n",
    "Pandas functions and/or methods are used to assess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['tweet_id'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['in_reply_to_status_id'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['retweeted_status_id'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_unique_values=numerator_count.index.unique()\n",
    "numerator_unique_values\n",
    "\n",
    "index_list=[]\n",
    "for i in numerator_unique_values:\n",
    "    if i >15:\n",
    "        l=df[df['rating_numerator']==i].index\n",
    "        #print(str(list(l)))\n",
    "        index_list.append(list(l))\n",
    "print(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in index_list:\n",
    "    for j in i:\n",
    "        print(j,df.rating_numerator[j], df.text[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom_count=df.rating_denominator.value_counts()\n",
    "denom_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values=denom_count.index.unique()\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list=[]\n",
    "for i in range(1,len(unique_values)):\n",
    "    l=df[df['rating_denominator']==unique_values[i]].index\n",
    "    index_list.append(list(l))\n",
    "print(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in index_list:\n",
    "    for j in i:\n",
    "        print(j,df.tweet_id[j],df.rating_denominator[j], df.text[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[342]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(image_pred['tweet_id'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(image_pred['jpg_url'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_pred[image_pred['jpg_url'].duplicated(keep=False)==True]\n",
    "pd.concat(g for _, g in image_pred.groupby(\"jpg_url\") if len(g) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(image_pred['tweet_id'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_pred.query('p1_dog==True').p1.unique())\n",
    "print(image_pred.query('p2_dog==True').p2.unique())\n",
    "print(image_pred.query('p3_dog==True').p3.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data.friends_count.nunique()\n",
    "##friends column are not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data.followers_count.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='observation'></a>\n",
    "### 3.3 Observations\n",
    "\n",
    "**WeRateDogs Twitter Archive**\n",
    "\n",
    "- **doggo, floofer,pupper,puppo** columns are not True/False values. Actual doggo, floofer, pupper, puppo stage names are exist.\n",
    "- Retweets are may not be used for analysis.\n",
    "- **retweeted_status_id,retweeted_status_user_id,in_reply_to_status_id,in_reply_to_user_id**s non null values are float values.\n",
    "- Wiered ratings observed in **rating_numerator & ratings_denominator**: \n",
    "    + No clues for actual ratings (666/10, 182/10, 1776/10, All time 24/7, Date 11/15/15, 20/10, snoop dog 420/10, 4/20(tweet id: 686035780142297088))\n",
    "    + Only part of decimal numbers were extracted for numerator(11.27/10, 9.75/10, 11.26/10)\n",
    "    + Ratings for Multiple dogs in a image get aggreated ratings (44/40,50/50, 165/150, 84/70,88/80, 144/120,143/130,45/50,99/90, 121/110, 204/170) \n",
    "    + Extracted duplicated OO/OO format in text column (Current value --> Updated value) \n",
    "      (Event 9/11--> 14/10,Size3 1/2 legged --> 9/10, 50/50 --> 11/10, 17/10 --> 13/10, 960/00 -->13/10, 4/20 --> 13/10) \n",
    "\n",
    "**Image_prediction**\n",
    "- Duplicated image predictions (66 duplicates)\n",
    "- Images with multiple dogs \n",
    "\n",
    "**API DATA**\n",
    "- **friend count** is not real data (Twitter limitation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## 4. Clean\n",
    "\n",
    "Based on Observation in previous Step, I have cleaned the data for quality and tidiness.Also I have combined all three datasets into one dataframe so that I can do more analysis in the next step.\n",
    "The steps that I have gone through are belows.\n",
    "\n",
    " + 4.1 Tiwtter_Archive: Delete Retweets\n",
    " + 4.2 Twitter_Archive: Drop columns that are not used\n",
    " + 4.3 Twitter_Archive: Create dogs stage columns and drop doggo,floofer,pupper, puppo\n",
    " + 4.4 Twitter_archive: Create Year, Month, Day colums from timestamp\n",
    " + 4.5 Twitter_Archive: Correct values of ratings_numberator & ratings_denominator\n",
    " + 4.6 Image_Prediction: Drop duplicated Image prediction based on url\n",
    " + 4.7 Image_prediction: Create 1 column for image prediction and 1 column for confidence level\n",
    " + 4.8. Image_prediction:  Delete columns that are not used\n",
    " + 4.9 API-DATA: Change type for tweet_id\n",
    " + 4.10 API-DATA: Drop Friends_count, retweeted column\n",
    " + 4.11 Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the tables before cleaning\n",
    "df_clean = df.copy()\n",
    "image_pred_clean = image_pred.copy()\n",
    "api_data_clean = api_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tiwtter_Archive: Delete Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Delete retweets by filtering the NaN of retweeted_status_user_id\n",
    "df_clean = df_clean[pd.isnull(df_clean['retweeted_status_user_id'])]\n",
    "\n",
    "#TEST\n",
    "print(sum(df_clean.retweeted_status_user_id.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Twitter_Archive: Drop columns that are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the column names of twitter_archive_clean\n",
    "print(df_clean.columns)\n",
    "\n",
    "#CODE: Delete columns no needed\n",
    "df_clean = df_clean.drop(['source','in_reply_to_status_id','in_reply_to_user_id',\n",
    "                           'retweeted_status_id','retweeted_status_user_id', \n",
    "                            'retweeted_status_timestamp', 'expanded_urls'], 1)                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Twitter_Archive: Create dogs stage columns and drop doggo,floofer,pupper, puppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Melt the doggo, floofer, pupper and puppo columns to dogs and dogs_stage column\n",
    "df_clean = pd.melt(df_clean, id_vars=['tweet_id','timestamp','text','rating_numerator',\n",
    "                                       'rating_denominator','name'],                                                            \n",
    "                               var_name='dogs', value_name='dogs_stage')\n",
    "\n",
    "#CODE: drop dogs\n",
    "df_clean = df_clean.drop('dogs', 1)\n",
    "\n",
    "#CODE: Sort by dogs_stage then drop duplicated based on tweet_id except the last occurrence\n",
    "df_clean = df_clean.sort_values('dogs_stage').drop_duplicates(subset='tweet_id', keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "df_clean['dogs_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Twitter_archive: Create Year, Month, Day colums from timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: convert timestamp to datetime\n",
    "df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n",
    "\n",
    "#extract year, month and day to new columns\n",
    "df_clean['year'] = df_clean['timestamp'].dt.year\n",
    "df_clean['month'] = df_clean['timestamp'].dt.month\n",
    "df_clean['day'] = df_clean['timestamp'].dt.day\n",
    "\n",
    "#Finally drop timestamp column\n",
    "df_clean = df_clean.drop('timestamp', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Twitter_Archive: Correct values of ratings_numberator & ratings_denominator\n",
    "\n",
    "#### 4.5.1 Delete uncorrect ratings which I cannot get clue for actual values. \n",
    "- No clues for actual ratings \n",
    "+ 666/10\n",
    "+ 182/10\n",
    "+ 1776/10 \n",
    "+ All time 24/7\n",
    "+ Date 11/15/15\n",
    "+ 20/10\n",
    "+ snoop dog 420/10\n",
    "+ 4/20(tweet id: 686035780142297088))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Code: Check number of rows before deleting rows\n",
    "print(\"# of Rows before drop:\",df_clean.shape[0])\n",
    "#Code: Check index with above values\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    print(df_clean.text[df_clean.query('rating_numerator==666 and rating_denominator==10').index[0]])\n",
    "    print(df_clean.text[df_clean.query('rating_numerator==182 and rating_denominator==10').index[0]])\n",
    "    print(df_clean.text[df_clean.query('rating_numerator==1776 and rating_denominator==10').index[0]])\n",
    "    print(df_clean.text[df_clean.query('rating_numerator==24 and rating_denominator==7').index[0]])\n",
    "    print(df_clean.text[df_clean.query('rating_numerator==11 and rating_denominator==15').index[0]])\n",
    "    print(df_clean.text[df_clean.query('rating_numerator==20 and rating_denominator==16').index[0]])\n",
    "    print(df_clean.text[df_clean.query('rating_numerator==420 and rating_denominator==10').index[0]])\n",
    "    print(df_clean.text[df_clean.query('rating_numerator==420 and rating_denominator==10').index[1]])\n",
    "    print(df_clean.text[df_clean.query('tweet_id==686035780142297088').index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code: delete rows with wired values \n",
    "df_clean.drop(df_clean.query('rating_numerator==666 and rating_denominator==10').index[0],inplace=True)\n",
    "df_clean.drop(df_clean.query('rating_numerator==182 and rating_denominator==10').index[0], inplace=True)\n",
    "df_clean.drop(df_clean.query('rating_numerator==1776 and rating_denominator==10').index[0],inplace=True)\n",
    "df_clean.drop(df_clean.query('rating_numerator==24 and rating_denominator==7').index[0],inplace=True)\n",
    "df_clean.drop(df_clean.query('rating_numerator==11 and rating_denominator==15').index[0],inplace=True)\n",
    "df_clean.drop(df_clean.query('rating_numerator==20 and rating_denominator==16').index[0],inplace=True)\n",
    "df_clean.drop(df_clean.query('rating_numerator==420 and rating_denominator==10').index[0],inplace=True)\n",
    "df_clean.drop(df_clean.query('rating_numerator==420 and rating_denominator==10').index[0],inplace=True)\n",
    "df_clean.drop(df_clean.query('tweet_id==686035780142297088').index[0],inplace=True)\n",
    "\n",
    "\n",
    "#TEST: see 8 rows are deleted\n",
    "df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 Update  miscaptured rating values with actual ratings\n",
    "\n",
    "**Current value --> Updated value** <br/>\n",
    "+ Event 9/11--> 14/10\n",
    "+ Size3 1/2 legged --> 9/10\n",
    "+ 50/50 --> 11/10\n",
    "+ 17/10 --> 13/10\n",
    "+ 960/00 -->13/10\n",
    "+ 4/20 --> 13/10\n",
    "+ 7/11 -->10/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code: Find location for ratings above\n",
    "error_list=[]\n",
    "t=df_clean.query('rating_numerator==9 and rating_denominator==11').index[0]\n",
    "error_list.append(t)\n",
    "t=df_clean.query('rating_numerator==7 and rating_denominator==11').index[0]\n",
    "error_list.append(t)\n",
    "t=df_clean.query('rating_numerator==1 and rating_denominator==2').index[0]\n",
    "error_list.append(t)\n",
    "t=df_clean.query('rating_numerator==50 and rating_denominator==50').index[0]\n",
    "error_list.append(t)\n",
    "t=df_clean.query('rating_numerator==17 and rating_denominator==10').index[0]\n",
    "error_list.append(t)\n",
    "t=df_clean.query('rating_numerator==960 and rating_denominator==00').index[0]\n",
    "error_list.append(t)\n",
    "t=df_clean.query('rating_numerator==4 and rating_denominator==20').index[0]\n",
    "error_list.append(t)\n",
    "print(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code: find proper value(second set value extracted by regular expressions) from text and update numerator\n",
    "#error_list=[3065,2154,3199,52,2438,3594]\n",
    "regex = re.compile(r\"(\\d+\\/\\d+)\")\n",
    "\n",
    "for i in error_list:\n",
    "    t=df_clean.text[i]   \n",
    "    numerator=regex.findall(t)[1].split('/')[0]\n",
    "    denominator=regex.findall(t)[1].split('/')[1]\n",
    "    print(i,t,numerator,denominator)\n",
    "    df_clean.loc[(df_clean.index==i),'rating_numerator']=numerator\n",
    "    df_clean.loc[(df_clean.index==i),'rating_denominator']=denominator\n",
    "\n",
    "#TEST: cehck replaced values\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(df_clean[df_clean.index.isin(error_list)==True][['text','rating_numerator','rating_denominator']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 Change ratings columns to float values types and correct numerator values with decial point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code: change int type to float\n",
    "df_clean[['rating_numerator', 'rating_denominator']] = df_clean[['rating_numerator','rating_denominator']].astype(float)\n",
    "\n",
    "# Test: check types of ratins\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Check & Update correct numerator for decimal values \n",
    "import re\n",
    "regex = re.compile(r\"(\\d+\\.\\d*\\/\\d+)\")\n",
    "\n",
    "\n",
    "index_list=df_clean[df_clean['text'].str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")==True].index\n",
    "for i in index_list:\n",
    "    t=df_clean.text[i]   \n",
    "    numerator=regex.findall(t)[0].split('/')[0]\n",
    "    print(i,t,numerator)\n",
    "    df_clean.loc[(df_clean.index==i),'rating_numerator']=float(numerator)\n",
    "    #print(df_clean.text[i].str.extract('(\\d+\\.\\d*\\/\\d+)',expand=True).loc[i])\n",
    "\n",
    "\n",
    "#TEST\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(df_clean[df_clean['text'].str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")]\n",
    "            [['tweet_id', 'text', 'rating_numerator', 'rating_denominator']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 Change aggregated ratings for multiple dos in a single images\n",
    "\n",
    "Look for values of 44/40,50/50, 165/150, 84/70,88/80, 144/120,143/130,45/50,99/90, 121/110, 204/170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom_count=df_clean.rating_denominator.value_counts()\n",
    "denom_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df_clean[df_clean.rating_denominator!=10.0][['rating_numerator','rating_denominator']]\n",
    "index_list=list(df_temp.index)\n",
    "#print(index_list)\n",
    "df_temp['num_dogs']=df_temp.rating_denominator/10\n",
    "df_temp['new_rating_numerator']=df_temp.rating_numerator/df_temp.num_dogs\n",
    "df_temp['new_rating_numerator']=df_temp['new_rating_numerator'].astype(float)\n",
    "\n",
    "df_clean.loc[(df_clean.rating_denominator!=10.0), 'rating_numerator']=df_temp.new_rating_numerator\n",
    "df_clean.loc[(df_clean.rating_denominator!=10.0), 'rating_denominator']=10.0\n",
    "\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(df_clean[df_clean.index.isin(index_list)==True][['text','rating_numerator','rating_denominator']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Image_Prediction: Drop duplicated Image prediction based on url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Delete duplicated jpg_url\n",
    "image_pred_clean = image_pred_clean.drop_duplicates(subset=['jpg_url'], keep='last')\n",
    "\n",
    "#TEST\n",
    "sum(image_pred_clean['jpg_url'].duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Image_prediction: Create 1 column for image prediction and 1 column for confidence level\n",
    "\n",
    "Create a function where I keep the first true prediction along the confidence level as new columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: the first true prediction (p1, p2 or p3) will be store in these lists\n",
    "dog_type = []\n",
    "confidence_list = []\n",
    "\n",
    "#create a function with nested if to capture the dog type and confidence level\n",
    "# from the first 'true' prediction\n",
    "def image(image_pred_clean):\n",
    "    if image_pred_clean['p1_dog'] == True:\n",
    "        dog_type.append(image_pred_clean['p1'])\n",
    "        confidence_list.append(image_pred_clean['p1_conf'])\n",
    "    elif image_pred_clean['p2_dog'] == True:\n",
    "        dog_type.append(image_pred_clean['p2'])\n",
    "        confidence_list.append(image_pred_clean['p2_conf'])\n",
    "    elif image_pred_clean['p3_dog'] == True:\n",
    "        dog_type.append(image_pred_clean['p3'])\n",
    "        confidence_list.append(image_pred_clean['p3_conf'])\n",
    "    else:\n",
    "        dog_type.append('None')\n",
    "        confidence_list.append('None')\n",
    "\n",
    "#series objects having index the image_pred_clean column.        \n",
    "image_pred_clean.apply(image, axis=1)\n",
    "\n",
    "#create new columns\n",
    "image_pred_clean['dog_type'] = dog_type\n",
    "image_pred_clean['confidence_list'] = confidence_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that has prediction_list 'error'\n",
    "image_pred_clean = image_pred_clean[image_pred_clean['dog_type'] != 'None']\n",
    "\n",
    "#TEST: \n",
    "image_pred_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Image_prediction:  Delete columns that are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: print list of image_prediction columns\n",
    "print(list(image_pred_clean))\n",
    "\n",
    "#Delete columns\n",
    "image_pred_clean = image_pred_clean.drop(['img_num', 'p1', \n",
    "                                                      'p1_conf', 'p1_dog', \n",
    "                                                      'p2', 'p2_conf', \n",
    "                                                      'p2_dog', 'p3', \n",
    "                                                      'p3_conf', \n",
    "                                                      'p3_dog'], 1)\n",
    "\n",
    "#TEST\n",
    "image_pred_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pred_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 API-DATA: Change type for tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: change tweet_id from str to int\n",
    "api_data_clean['tweet_id'] = api_data_clean['tweet_id'].astype(int)\n",
    "\n",
    "#TEST\n",
    "api_data_clean['tweet_id'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 API-DATA: Drop Friends_count, retweeted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Delete retweeted, friends_count column\n",
    "print(api_data_clean.columns)\n",
    "api_data_clean=api_data_clean.drop(columns=['retweeted','friends_count'])\n",
    "#TEST\n",
    "print(api_data_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: create a new dataframe that merge df_clean and image_pred\n",
    "dfs = pd.merge(df_clean, \n",
    "                      image_pred_clean, \n",
    "                      how = 'left', on = ['tweet_id'])\n",
    "\n",
    "#keep rows that have picture (jpg_url)\n",
    "dfs = dfs[dfs['jpg_url'].notnull()]\n",
    "\n",
    "#TEST\n",
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: create a new dataframe that merge dfs and api_data\n",
    "df_twitter = pd.merge(dfs, api_data_clean, \n",
    "                      how = 'left', on = ['tweet_id'])\n",
    "\n",
    "#TEST\n",
    "df_twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.12 Save cleaned data in \"twitter_archive_master.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the clean DataFrame in a CSV file\n",
    "df_twitter.to_csv('twitter_archive_master.csv', \n",
    "                 index=False, encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
